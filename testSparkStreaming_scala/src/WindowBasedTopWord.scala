import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.ReceiverInputDStream
import org.apache.spark.streaming.{Seconds, Durations, StreamingContext}

/**
  * Created by root on 2016/7/22 0024.
  */
class WindowBasedTopWord {

}
object WindowBasedTopWord{
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("WindowBasedTopWord").setMaster("local[2]")
    val ssc = new StreamingContext(conf,Durations.seconds(5)) //这里的5秒是指切分RDD的间隔
    ssc.checkpoint("hdfs://chenkl/user/wordcount_checkpoint") //设置docheckpoint目录,没有会自动创建


    val words: ReceiverInputDStream[String] = ssc.socketTextStream("master",9999)
    val pairs = words.flatMap(_.split(" ")).map(x => (x(1),1))
    pairs.foreachRDD(rdd => {
      println("--------------split RDD begin--------------")
      rdd.foreach(println)
      println("--------------split RDD end--------------")
    })
    /*
    reduceByKeyAndWindow(reduceFunc,invReduceFunc,windowDuration,slideDuration)
    reduceFunc：用于计算window框住的RDDS
    invReduceFunc：用于优化的函数，减少window滑动中去计算重复的数据，通过“_-_”即可优化
    windowDuration：表示window框住的时间长度，如本例5秒切分一次RDD，框20秒，就会保留最近4次切分的RDD
    slideDuration：表示window滑动的时间长度，即每隔多久执行本计算


    本例5秒切分一次RDD，每次滑动10秒，window框住20秒的RDDS，即：每10秒计算最近20秒切分的RDDS，中间有10秒重复，
    通过invReduceFunc参数进行去重优化
     */
    val pairsWindow = pairs.reduceByKeyAndWindow(_+_,_-_,Durations.seconds(20),Durations.seconds(10))
    val sortDstream = pairsWindow.transform(rdd => {
      val sortRdd = rdd.map(t => (t._2,t._1)).sortByKey(false).map(t => (t._2,t._1))  //降序排序
      val more = sortRdd.take(3)  //取前3个输出
      println("--------------print top 3 begin--------------")
      more.foreach(println)
      println("--------------print top 3 end--------------")
      sortRdd
    })
    sortDstream.print()


    ssc.start()
    ssc.awaitTermination()
  }
}





object TestWordCount{
  def main(args: Array[String]) {

    // Create the context with a 1 second batch size
    // 创建SparkConf实例
    val sparkConf = new SparkConf().setAppName("NetworkWordCount")

    ///创建Spark Streaming Context，每隔1秒钟处理一批数据，那么这一秒收集的数据存放在哪，如何将收集的数据推送出去？是生产者主动推出去还是消费者每隔1秒钟来拉取一次数据
    val ssc = new StreamingContext(sparkConf, Seconds(1))

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream("master", 9999, StorageLevel.MEMORY_AND_DISK_SER)
    //flatMap是把将每一行使用空格做分解，那么words对应的数据结构是怎么样的？
    ///words是个集合，每个集合元素依然是个集合，这个集合存放单词
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    //启动计算作业
    ssc.start()

    //等待结束，什么时候结束作业，即触发什么条件会让作业执行结束
    ssc.awaitTermination()
  }
}
